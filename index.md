---
layout: page
title: Jerry Chee 
tagline: Department of Computer Science, Cornell University  
description: Jerry Chee Personal Site
---
![image](./assets/JChee_headshot_fa22.jpg){: height="160px" width="160px"}

**Email:** JerryChee [at] cs.cornell.edu

[//]: # ([CV](assets/JerryChee_CV.pdf))
[CV](assets/JerryChee_CV.pdf) 
/ 
[Google Scholar](https://scholar.google.com/citations?user=qyQpUAkAAAAJ&hl=en)
/
[ArXiv](https://arxiv.org/a/chee_j_1)

<span style="color: black">
If you have a use case for machine learning, please reach out!
</span>

---

#### About
I am a Ph.D. student in Computer Science at Cornell University, advised by [Chris De Sa](https://www.cs.cornell.edu/~cdesa/).
I am interested in making machine learning work for practitioners.
I have collaborated with industry and scientific practitioners across a diverse array of fields, including deep noise suppression at Microsoft, economic causal modeling at Amazon, safe recommender systems at Meta, and plant geneticists at Cornell. 
I have developed novel methodologies across deep learning compression, scalable statistical inference, safe recommender systems, etc.

In my previous professional life I worked as a data scientist consultant at McKinsey & Company.
I graduated from the University of Chicago in 2017 with a degree in Computational and Applied Mathematics, where I worked with [Panos Toulis](https://www.chicagobooth.edu/faculty/directory/t/panagiotis-toulis-panos) at UChicago Booth on statistically motivated topics in stochastic gradient descent (SGD). 

---
#### Publications
**"Plus/minus the learning rate": Easy and Scalable Statistical Inference with SGD**\\
Jerry Chee, Hwanwoo Kim, Panos Toulis\\
*In AISTATS 2023* 

**Model Preserving Compression for Neural Networks**\\
Jerry Chee, Megan Renz, Anil Damle, Chris De Sa\\
*In NeurIPS 2022*\\
[[Proceedings](https://openreview.net/pdf?id=gt-l9Hu2ndd)] [[ArXiv](https://arxiv.org/abs/2108.00065)]

**Performance Optimizations on U-Net Speech Enhancement Models**\\
Jerry Chee, Sebastian Braun, Vishak Gopal, Ross Cutler\\
*In IEEE MMSP 2022*\\
[[ArXiv](https://arxiv.org/abs/2110.04378)]

**How Low Can We Go: Trading Memory for Error in Low-Precision Training**\\
Chengrun Yang, Ziyang Wu, Jerry Chee, Chris De Sa, Madeleine Udell\\
*In ICLR 2022*\\
[[Proceedings](https://openreview.net/pdf?id=YpSxqy_RE84)] [[ArXiv](https://arxiv.org/abs/2106.09686)]

**Understanding and Detecting Convergence for Stochastic Gradient Descent**\\
Jerry Chee, Ping Li\\
*In IEEE Big Data 2020*\\
[[Proceedings](https://ieeexplore.ieee.org/document/9378129)] [[ArXiv](https://arxiv.org/abs/2008.12224)]

**Convergence diagnostics for stochastic gradient descent with constant step size**\\
Jerry Chee, Panos Toulis\\
*In AISTATS 2018, **<span style="color: red">Oral Presentation (5%)</span>***\\
[[Proceedings](http://proceedings.mlr.press/v84/chee18a/chee18a.pdf)] [[ArXiv](https://arxiv.org/abs/1710.06382)]


---
#### Talks
* JSM, Denver, USA, 2019 \\
Topic-contributed paper session \\
Statistical properties of stochastic gradient descent.

* AISTATS, Lanzarote, Canary Islands, 2018 \\
Oral Presentation \\
Convergence Diagnostics for SGD with constant stepsize.
